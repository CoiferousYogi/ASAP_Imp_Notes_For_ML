{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability and statistics are foundational concepts in data science, providing the framework for understanding uncertainty, variability, and randomness in data.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here's an overview of how probability and statistics are used in data science:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Probability Theory**:\n",
    "   - **Probability Basics**: Probability theory deals with quantifying uncertainty and the likelihood of events occurring. It provides a formal framework for reasoning about uncertain outcomes.\n",
    "   <img src=\"image/1.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>\n",
    "   - **Probability Distributions**: Probability distributions describe the likelihood of different outcomes in a random experiment. Common distributions used in data science include the normal (Gaussian), binomial, Poisson, and exponential distributions.\n",
    "   <img src=\"image/2.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>\n",
    "   - **Joint and Conditional Probability**: Joint probability and conditional probability are fundamental concepts used to analyze the relationships between multiple random variables.\n",
    "   <img src=\"image/3.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>\n",
    "   - **Bayesian Probability**: Bayesian probability is a framework for updating beliefs about uncertain events based on new evidence. It is widely used in statistical inference and machine learning, particularly in Bayesian statistics and Bayesian inference.\n",
    "   <img src=\"image/4.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Descriptive Statistics**:\n",
    "    - **Measures of Central Tendency**: Descriptive statistics such as mean, median, and mode are used to summarize the central tendency of a dataset.\n",
    "    <img src=\"image/5.png\" style=\"width:500px;height:250px;\">\n",
    "    <br>\n",
    "   <img src=\"image/6.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>\n",
    "    - **Measures of Dispersion**: Descriptive statistics like variance, standard deviation, and range quantify the spread or dispersion of data points around the central tendency.\n",
    "   <img src=\"image/7.png\" style=\"width:500px;height:250px;\">\n",
    "   <br>\n",
    "    - **Skewness and Kurtosis**: Skewness measures the asymmetry of the distribution, while kurtosis quantifies the peakedness or flatness of the distribution.\n",
    "   <img src=\"image/8.png\" style=\"width:300px;height:250px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Inferential Statistics**:\n",
    "   - **Hypothesis Testing**: Hypothesis testing is a statistical method used to make inferences about population parameters based on sample data. Common tests include t-tests, chi-square tests, ANOVA, and z-tests.\n",
    "   <img src=\"image/9.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>\n",
    "   - **Confidence Intervals**: Confidence intervals provide a range of values within which the true population parameter is likely to lie, with a specified level of confidence.\n",
    "   <img src=\"image/10.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>\n",
    "   - **Regression Analysis**: Regression analysis is used to model the relationship between a dependent variable and one or more independent variables. It is widely used for prediction and inference in data science.\n",
    "   <img src=\"image/11.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Probability Models and Statistical Inference**:\n",
    "   - **Maximum Likelihood Estimation (MLE)**: MLE is a method for estimating the parameters of a probability distribution that maximize the likelihood of the observed data.\n",
    "   <img src=\"image/12.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>\n",
    "   - **Bayesian Inference**: Bayesian inference is a probabilistic framework for updating beliefs about model parameters based on observed data and prior knowledge.\n",
    "   <img src=\"image/13.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Sampling Methods**:\n",
    "   - **Random Sampling**: Random sampling is a method for selecting a subset of individuals or observations from a population in such a way that each member has an equal chance of being selected.\n",
    "    <img src=\"image/14.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>\n",
    "   \n",
    "    - **Stratified Sampling**: Stratified sampling involves dividing the population into homogeneous subgroups (strata) and then sampling from each subgroup independently.\n",
    "    <img src=\"image/15.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>\n",
    "   - **Bootstrapping**: Bootstrapping is a resampling technique used to estimate the sampling distribution of a statistic by repeatedly sampling with replacement from the observed data.\n",
    "   <img src=\"image/16.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **Probabilistic Graphical Models**:\n",
    "   - **Bayesian Networks**: Bayesian networks are graphical models that represent probabilistic relationships between variables using directed acyclic graphs (DAGs). They are used for probabilistic reasoning and decision-making in uncertain environments.\n",
    "   <img src=\"image/17.png\" style=\"width:500px;height:300px;\">\n",
    "   <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data science, probability and statistics are applied across various stages of the data analysis pipeline, from data exploration and preprocessing to model building, evaluation, and interpretation. A solid understanding of these concepts is essential for making sound decisions and drawing meaningful insights from data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
